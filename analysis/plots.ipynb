{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "\n",
    "from adjustText import adjust_text\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim\n",
    "from tqdm.auto import tqdm\n",
    "import umap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cooc_r0_0002526.json', 'r') as f:\n",
    "    obj = json.load(f)\n",
    "cooc = np.array(obj['binary_counts'])\n",
    "names = obj['names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram the log-cooccurrences to figure out a good\n",
    "# cutoff for the weighting function.\n",
    "coocs = cooc.flatten()\n",
    "coocs = coocs[np.nonzero(coocs)]\n",
    "plt.hist(np.log10(coocs))\n",
    "plt.xlabel('log_10 cooccurrence')\n",
    "plt.ylabel('count')\n",
    "plt.show()\n",
    "\n",
    "# Select a weighting cutoff based on a percentile.\n",
    "x_max = sorted(coocs)[len(coocs) // 2]\n",
    "print('selected x_max as', x_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train GloVe-style embeddings.\n",
    "\n",
    "n_stores = len(names)\n",
    "n_feats = 16\n",
    "bias_lr_boost = math.sqrt(n_feats)\n",
    "features = nn.Parameter(torch.randn(n_stores, n_feats)*0.1)\n",
    "features_bias = nn.Parameter(torch.zeros(n_stores))\n",
    "contexts = nn.Parameter(torch.randn(n_stores, n_feats)*0.1)\n",
    "contexts_bias = nn.Parameter(torch.zeros(n_stores))\n",
    "cooc_matrix = torch.tensor(cooc).to(features)\n",
    "\n",
    "def glove_loss(alpha=0.75):\n",
    "    pred = (features @ contexts.T) + (features_bias[:, None] + contexts_bias) * bias_lr_boost\n",
    "    targ = torch.where(cooc_matrix == 0, 0.0, torch.log(cooc_matrix))\n",
    "    weights = (cooc_matrix.clamp(max=x_max) / x_max) ** alpha\n",
    "    weights -= torch.diag(torch.diag(weights))\n",
    "    return (weights * (pred - targ)**2).mean()\n",
    "\n",
    "num_iters = 20000\n",
    "opt = torch.optim.Adam([features, features_bias, contexts, contexts_bias], lr=1e-2)\n",
    "lrs = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=num_iters)\n",
    "losses = []\n",
    "pbar = tqdm(range(num_iters))\n",
    "for _ in pbar:\n",
    "    loss = glove_loss()\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    lrs.step()\n",
    "    losses.append(loss.item())\n",
    "    pbar.set_description(f\"loss {loss.item():7.05}\")\n",
    "\n",
    "features = features.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.ylim(min(losses), sorted(losses)[round(len(losses)*0.9)])\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reducer = umap.UMAP()\n",
    "embedding = reducer.fit_transform(features)\n",
    "embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 12))\n",
    "ax.scatter(embedding[:, 0], embedding[:, 1])\n",
    "texts = [plt.text(embedding[i, 0], embedding[i, 1], name, ha='center', va='center') for i, name in enumerate(names)]\n",
    "adjust_text(texts, arrowprops=dict(arrowstyle='->', color='red'))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking up neighbors using co-occurrences alone doesn't work well.\n",
    "# Example: McDonald's has tons of locations, so chances are it will\n",
    "# be near just about anything.\n",
    "\n",
    "correlation = cooc @ cooc.T\n",
    "correlation /= np.sqrt(np.diag(correlation)[None, :] @ np.diag(correlation)[:, None])\n",
    "\n",
    "def frequent_neighbors(store):\n",
    "    idx = names.index(store)\n",
    "    row = cooc[idx].copy()\n",
    "    store_count = row[idx]\n",
    "    row[idx] = 0\n",
    "    print(f'Frequent neighbors for \"{store}\" ({store_count} locations)')\n",
    "    indices = np.argsort(row)[::-1][:5]\n",
    "    for i in indices:\n",
    "        print(f\"{names[i]}: {100*(row[i] / store_count):.02f}%\")\n",
    "    print('----------------')\n",
    "\n",
    "frequent_neighbors(\"Sephora\")\n",
    "frequent_neighbors(\"McDonald's\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking up neighbors using embeddings.\n",
    "\n",
    "norm_features = features / np.linalg.norm(features, axis=-1, keepdims=True)\n",
    "def frequent_neighbors(store):\n",
    "    idx = names.index(store)\n",
    "    dots = norm_features @ norm_features[idx]\n",
    "    dots[idx] = 0\n",
    "    print(f'Cosine neighbors for \"{store}\"')\n",
    "    indices = np.argsort(dots)[::-1][:5]\n",
    "    for i in indices:\n",
    "        print(f\"{names[i]}: dot product {dots[i]}\")\n",
    "    print('----------------')\n",
    "\n",
    "# frequent_neighbors(\"Sephora\")\n",
    "frequent_neighbors(\"McDonald's\")\n",
    "frequent_neighbors(\"Apple\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
